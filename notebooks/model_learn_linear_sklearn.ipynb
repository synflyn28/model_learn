{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74l7lcFQk4kT"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixh2Tyl1FHaj"
   },
   "source": [
    "In this first cell we''ll load the necessary libraries and setup some logging and display options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaCENoitkiXK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that displays the contents of a NetCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_netcdf(nc_path):\n",
    "    \n",
    "    # Open NetCDF as xarray DataSet and print the info attribute.\n",
    "    print(xr.open_dataset(nc_path).info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that creates a copy of a NetCDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netcdf_copy(path_orig,\n",
    "                path_copy):\n",
    "    \n",
    "    # Open the NetCDF as an xarray DataSet and write it back out as NetCDF.\n",
    "    xr.open_dataset(path_orig, decode_times=False).to_netcdf(path=path_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the labels NetCDF into a corresponding predicted labels NetCDF. We expect to update the label variable's values in this NetCDF with values predicted by an AI model that we'll develop and train below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Dataset.info of <xarray.Dataset>\n",
      "Dimensions:       (ilev: 27, lat: 12, lev: 26, lon: 23, nbnd: 2, slat: 90, slon: 180, time: 720)\n",
      "Coordinates:\n",
      "  * ilev          (ilev) float64 2.194 4.895 9.882 18.05 29.84 44.62 61.61 ...\n",
      "  * lat           (lat) float64 -90.0 -74.0 -58.0 -42.0 -26.0 -10.0 6.0 22.0 ...\n",
      "  * lev           (lev) float64 3.545 7.389 13.97 23.94 37.23 53.11 70.06 ...\n",
      "  * lon           (lon) float64 0.0 16.0 32.0 48.0 64.0 80.0 96.0 112.0 ...\n",
      "  * slat          (slat) float64 -89.0 -87.0 -85.0 -83.0 -81.0 -79.0 -77.0 ...\n",
      "  * slon          (slon) float64 -1.0 1.0 3.0 5.0 7.0 9.0 11.0 13.0 15.0 ...\n",
      "  * time          (time) datetime64[ns] 2000-12-27 2000-12-27T00:30:00 ...\n",
      "Dimensions without coordinates: nbnd\n",
      "Data variables:\n",
      "    P0            float64 ...\n",
      "    PTTEND        (time, lev, lat, lon) float32 ...\n",
      "    PUTEND        (time, lev, lat, lon) float32 ...\n",
      "    PVTEND        (time, lev, lat, lon) float32 ...\n",
      "    ch4vmr        (time) float64 ...\n",
      "    co2vmr        (time) float64 ...\n",
      "    date          (time) int32 ...\n",
      "    date_written  (time) |S8 ...\n",
      "    datesec       (time) int32 ...\n",
      "    f11vmr        (time) float64 ...\n",
      "    f12vmr        (time) float64 ...\n",
      "    gw            (lat) float64 ...\n",
      "    hyai          (ilev) float64 ...\n",
      "    hyam          (lev) float64 ...\n",
      "    hybi          (ilev) float64 ...\n",
      "    hybm          (lev) float64 ...\n",
      "    mdt           int32 ...\n",
      "    n2ovmr        (time) float64 ...\n",
      "    nbdate        int32 ...\n",
      "    nbsec         int32 ...\n",
      "    ndbase        int32 ...\n",
      "    ndcur         (time) int32 ...\n",
      "    nlon          (lat) int32 ...\n",
      "    nsbase        int32 ...\n",
      "    nscur         (time) int32 ...\n",
      "    nsteph        (time) int32 ...\n",
      "    ntrk          int32 ...\n",
      "    ntrm          int32 ...\n",
      "    ntrn          int32 ...\n",
      "    sol_tsi       (time) float64 ...\n",
      "    time_bnds     (time, nbnd) float64 ...\n",
      "    time_written  (time) |S8 ...\n",
      "    w_stag        (slat) float64 ...\n",
      "    wnummax       (lat) int32 ...\n",
      "Attributes:\n",
      "    Conventions:      CF-1.0\n",
      "    source:           CAM\n",
      "    case:             fv091x180L26_dry_HS\n",
      "    title:            CAM5-FV 2x2L26, dry HS\n",
      "    logname:          cjablono\n",
      "    host:             r1i3n29\n",
      "    Version:          $Name$\n",
      "    revision_Id:      $Id$\n",
      "    initial_file:     /glade2/scratch2/cjablono/fv091x180L26_dry_HS/fv091x180...\n",
      "    topography_file:  /glade/p/work/cjablono/dycore_initial_data/dcmip_james/...\n",
      "    history:          Fri Jul 13 04:02:27 2018: C:\\home\\miniconda3\\Library\\bi...\n",
      "    NCO:              netCDF Operators version 4.7.5 (Homepage = http://nco.s...>\n"
     ]
    }
   ],
   "source": [
    "# paths to the computed and predicted labels NetCDF files\n",
    "path_labels = 'C:/home/cam_learn/fv091x180L26_dry_HS.cam.h1.2000-12-27-00000_lowres.nc'\n",
    "path_predictions = 'C:/home/cam_learn/fv091x180L26_dry_HS.cam.h2.2000-12-27-00000_lowres.nc'\n",
    "\n",
    "# make a copy of the computed labels NetCDF as the label predictions NetCDF\n",
    "netcdf_copy(path_labels, path_predictions)\n",
    "display_netcdf(path_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0gBz25Glf-3"
   },
   "source": [
    "Next we'll load our flow variables (features) and time tendency forcings (labels) datasets into Xarray Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cC_-nNSlWIO"
   },
   "outputs": [],
   "source": [
    "path_features = 'C:/home/cam_learn/fv091x180L26_dry_HS.cam.h0.2000-12-27-00000_lowres.nc'\n",
    "ds_features = xr.open_dataset(path_features, decode_times=False)\n",
    "ds_labels = xr.open_dataset(path_labels, decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Dataset.info of <xarray.Dataset>\n",
       "Dimensions:       (ilev: 27, lat: 12, lev: 26, lon: 23, nbnd: 2, slat: 90, slon: 180, time: 720)\n",
       "Coordinates:\n",
       "  * ilev          (ilev) float64 2.194 4.895 9.882 18.05 29.84 44.62 61.61 ...\n",
       "  * lat           (lat) float64 -90.0 -74.0 -58.0 -42.0 -26.0 -10.0 6.0 22.0 ...\n",
       "  * lev           (lev) float64 3.545 7.389 13.97 23.94 37.23 53.11 70.06 ...\n",
       "  * lon           (lon) float64 0.0 16.0 32.0 48.0 64.0 80.0 96.0 112.0 ...\n",
       "  * slat          (slat) float64 -89.0 -87.0 -85.0 -83.0 -81.0 -79.0 -77.0 ...\n",
       "  * slon          (slon) float64 -1.0 1.0 3.0 5.0 7.0 9.0 11.0 13.0 15.0 ...\n",
       "  * time          (time) float64 0.0 0.02083 0.04167 0.0625 0.08333 0.1042 ...\n",
       "Dimensions without coordinates: nbnd\n",
       "Data variables:\n",
       "    P0            float64 ...\n",
       "    PS            (time, lat, lon) float32 ...\n",
       "    T             (time, lev, lat, lon) float32 ...\n",
       "    U             (time, lev, lat, lon) float32 ...\n",
       "    V             (time, lev, lat, lon) float32 ...\n",
       "    ch4vmr        (time) float64 ...\n",
       "    co2vmr        (time) float64 ...\n",
       "    date          (time) int32 ...\n",
       "    date_written  (time) |S8 ...\n",
       "    datesec       (time) int32 ...\n",
       "    f11vmr        (time) float64 ...\n",
       "    f12vmr        (time) float64 ...\n",
       "    gw            (lat) float64 ...\n",
       "    hyai          (ilev) float64 ...\n",
       "    hyam          (lev) float64 ...\n",
       "    hybi          (ilev) float64 ...\n",
       "    hybm          (lev) float64 ...\n",
       "    mdt           int32 ...\n",
       "    n2ovmr        (time) float64 ...\n",
       "    nbdate        int32 ...\n",
       "    nbsec         int32 ...\n",
       "    ndbase        int32 ...\n",
       "    ndcur         (time) int32 ...\n",
       "    nlon          (lat) int32 ...\n",
       "    nsbase        int32 ...\n",
       "    nscur         (time) int32 ...\n",
       "    nsteph        (time) int32 ...\n",
       "    ntrk          int32 ...\n",
       "    ntrm          int32 ...\n",
       "    ntrn          int32 ...\n",
       "    sol_tsi       (time) float64 ...\n",
       "    time_bnds     (time, nbnd) float64 ...\n",
       "    time_written  (time) |S8 ...\n",
       "    w_stag        (slat) float64 ...\n",
       "    wnummax       (lat) int32 ...\n",
       "Attributes:\n",
       "    Conventions:      CF-1.0\n",
       "    source:           CAM\n",
       "    case:             fv091x180L26_dry_HS\n",
       "    title:            CAM5-FV 2x2L26, dry HS\n",
       "    logname:          cjablono\n",
       "    host:             r1i3n29\n",
       "    Version:          $Name$\n",
       "    revision_Id:      $Id$\n",
       "    initial_file:     /glade2/scratch2/cjablono/fv091x180L26_dry_HS/fv091x180...\n",
       "    topography_file:  /glade/p/work/cjablono/dycore_initial_data/dcmip_james/...\n",
       "    history:          Fri Jul 13 04:01:48 2018: C:\\home\\miniconda3\\Library\\bi...\n",
       "    NCO:              netCDF Operators version 4.7.5 (Homepage = http://nco.s...>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_features.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Dataset.info of <xarray.Dataset>\n",
       "Dimensions:       (ilev: 27, lat: 12, lev: 26, lon: 23, nbnd: 2, slat: 90, slon: 180, time: 720)\n",
       "Coordinates:\n",
       "  * ilev          (ilev) float64 2.194 4.895 9.882 18.05 29.84 44.62 61.61 ...\n",
       "  * lat           (lat) float64 -90.0 -74.0 -58.0 -42.0 -26.0 -10.0 6.0 22.0 ...\n",
       "  * lev           (lev) float64 3.545 7.389 13.97 23.94 37.23 53.11 70.06 ...\n",
       "  * lon           (lon) float64 0.0 16.0 32.0 48.0 64.0 80.0 96.0 112.0 ...\n",
       "  * slat          (slat) float64 -89.0 -87.0 -85.0 -83.0 -81.0 -79.0 -77.0 ...\n",
       "  * slon          (slon) float64 -1.0 1.0 3.0 5.0 7.0 9.0 11.0 13.0 15.0 ...\n",
       "  * time          (time) float64 0.0 0.02083 0.04167 0.0625 0.08333 0.1042 ...\n",
       "Dimensions without coordinates: nbnd\n",
       "Data variables:\n",
       "    P0            float64 ...\n",
       "    PTTEND        (time, lev, lat, lon) float32 ...\n",
       "    PUTEND        (time, lev, lat, lon) float32 ...\n",
       "    PVTEND        (time, lev, lat, lon) float32 ...\n",
       "    ch4vmr        (time) float64 ...\n",
       "    co2vmr        (time) float64 ...\n",
       "    date          (time) int32 ...\n",
       "    date_written  (time) |S8 ...\n",
       "    datesec       (time) int32 ...\n",
       "    f11vmr        (time) float64 ...\n",
       "    f12vmr        (time) float64 ...\n",
       "    gw            (lat) float64 ...\n",
       "    hyai          (ilev) float64 ...\n",
       "    hyam          (lev) float64 ...\n",
       "    hybi          (ilev) float64 ...\n",
       "    hybm          (lev) float64 ...\n",
       "    mdt           int32 ...\n",
       "    n2ovmr        (time) float64 ...\n",
       "    nbdate        int32 ...\n",
       "    nbsec         int32 ...\n",
       "    ndbase        int32 ...\n",
       "    ndcur         (time) int32 ...\n",
       "    nlon          (lat) int32 ...\n",
       "    nsbase        int32 ...\n",
       "    nscur         (time) int32 ...\n",
       "    nsteph        (time) int32 ...\n",
       "    ntrk          int32 ...\n",
       "    ntrm          int32 ...\n",
       "    ntrn          int32 ...\n",
       "    sol_tsi       (time) float64 ...\n",
       "    time_bnds     (time, nbnd) float64 ...\n",
       "    time_written  (time) |S8 ...\n",
       "    w_stag        (slat) float64 ...\n",
       "    wnummax       (lat) int32 ...\n",
       "Attributes:\n",
       "    Conventions:      CF-1.0\n",
       "    source:           CAM\n",
       "    case:             fv091x180L26_dry_HS\n",
       "    title:            CAM5-FV 2x2L26, dry HS\n",
       "    logname:          cjablono\n",
       "    host:             r1i3n29\n",
       "    Version:          $Name$\n",
       "    revision_Id:      $Id$\n",
       "    initial_file:     /glade2/scratch2/cjablono/fv091x180L26_dry_HS/fv091x180...\n",
       "    topography_file:  /glade/p/work/cjablono/dycore_initial_data/dcmip_james/...\n",
       "    history:          Fri Jul 13 04:02:27 2018: C:\\home\\miniconda3\\Library\\bi...\n",
       "    NCO:              netCDF Operators version 4.7.5 (Homepage = http://nco.s...>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_labels.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the time variable in order to work out the initial date, number of steps, units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.IndexVariable 'time' (time: 720)>\n",
       "array([ 0.      ,  0.020833,  0.041667, ..., 14.9375  , 14.958333, 14.979167])\n",
       "Attributes:\n",
       "    long_name:  time\n",
       "    units:      days since 2000-12-27 00:00:00\n",
       "    calendar:   noleap\n",
       "    bounds:     time_bnds"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_features.variables['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we have the same time values for the targets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: time values match as expected\n"
     ]
    }
   ],
   "source": [
    "if (ds_features.variables['time'].values != ds_labels.variables['time'].values).any():\n",
    "    print('ERROR: Non-matching time values')\n",
    "else:\n",
    "    print(\"OK: time values match as expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function that creates a Series of timestamp values corresponding to the NetCDF's time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def extract_timestamps(ds,\n",
    "                       year,\n",
    "                       month,\n",
    "                       day):\n",
    "    \n",
    "    # Cook up an initial datetime object based on our specified initial date.\n",
    "    initial = datetime(2000, 12, 27)\n",
    "    \n",
    "    # Create an array of datetime objects from the time values (assumed to be in units of days since the inital date).\n",
    "    times = ds.variables['time'].values\n",
    "    datetimes = np.empty(shape=times.shape, dtype='datetime64[m]')\n",
    "    for i in range(datetimes.size):\n",
    "        datetimes[i] = initial + timedelta(days=times[i])\n",
    "\n",
    "    # Put the array into a Series and return it.    \n",
    "    return pd.Series(datetimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Series of datetime values from the DataSet's (NetCDF's) time coordinate variable's values.\n",
    "These will be used as time indices corresponding to feature and label values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2000-12-27 00:00:00\n",
       "1   2000-12-27 00:30:00\n",
       "2   2000-12-27 01:00:00\n",
       "3   2000-12-27 01:30:00\n",
       "4   2000-12-27 02:00:00\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps = extract_timestamps(ds_features, 2000, 12, 27)\n",
    "timestamps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature and target selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As features we'll use the following flow variables:\n",
    "\n",
    "* U (west-east (zonal) wind, m/s)\n",
    "* V (south-north (meridional) wind, m/s)\n",
    "* T (temperature, K)\n",
    "* PS (surface pressure, Pa)\n",
    "\n",
    "Time tendency forcings are the targets (labels) that our model should learn to predict.\n",
    "\n",
    "* PTTEND (time tendency forcing value corresponding to the temperature variable)\n",
    "* PUTEND (time tendency forcing value corresponding to zonal wind)\n",
    "* PVTEND (time tendency forcing value corresponding to meridional wind)\n",
    "\n",
    "Eventually we'll train/fit our model for an entire global 3-D grid, but for this example we'll select all lat/lon/time combinations for a single level (elevation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = pd.Series(ds_features.variables['PS'].values[:, :, :].flatten())\n",
    "t = pd.Series(ds_features.variables['T'].values[:, 0, :, :].flatten())\n",
    "u = pd.Series(ds_features.variables['U'].values[:, 0, :, :].flatten())\n",
    "v = pd.Series(ds_features.variables['V'].values[:, 0, :, :].flatten())\n",
    "pttend = pd.Series(ds_labels.variables['PTTEND'].values[:, 0, :, :].flatten())\n",
    "putend = pd.Series(ds_labels.variables['PUTEND'].values[:, 0, :, :].flatten())\n",
    "pvtend = pd.Series(ds_labels.variables['PVTEND'].values[:, 0, :, :].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to Pandas DataFrames containing inputs (features) and outputs (label/target) for use when predicting time tendency forcings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PS</th>\n",
       "      <th>T</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-12-27 00:00:00</th>\n",
       "      <td>101099.0625</td>\n",
       "      <td>210.862564</td>\n",
       "      <td>-0.814972</td>\n",
       "      <td>-0.280670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-27 00:30:00</th>\n",
       "      <td>101099.0625</td>\n",
       "      <td>210.862564</td>\n",
       "      <td>-0.706038</td>\n",
       "      <td>-0.494434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-27 01:00:00</th>\n",
       "      <td>101099.0625</td>\n",
       "      <td>210.862564</td>\n",
       "      <td>-0.542403</td>\n",
       "      <td>-0.669891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-27 01:30:00</th>\n",
       "      <td>101099.0625</td>\n",
       "      <td>210.862564</td>\n",
       "      <td>-0.336744</td>\n",
       "      <td>-0.793447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-27 02:00:00</th>\n",
       "      <td>101099.0625</td>\n",
       "      <td>210.862564</td>\n",
       "      <td>-0.104996</td>\n",
       "      <td>-0.855530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              PS           T         U         V\n",
       "timestamp                                                       \n",
       "2000-12-27 00:00:00  101099.0625  210.862564 -0.814972 -0.280670\n",
       "2000-12-27 00:30:00  101099.0625  210.862564 -0.706038 -0.494434\n",
       "2000-12-27 01:00:00  101099.0625  210.862564 -0.542403 -0.669891\n",
       "2000-12-27 01:30:00  101099.0625  210.862564 -0.336744 -0.793447\n",
       "2000-12-27 02:00:00  101099.0625  210.862564 -0.104996 -0.855530"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = pd.DataFrame({'timestamp': timestamps,\n",
    "                            'PS': ps,\n",
    "                            'T': t,\n",
    "                            'U': u,\n",
    "                            'V': v})\n",
    "df_features.set_index('timestamp', inplace=True)\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PTTEND</th>\n",
       "      <th>PUTEND</th>\n",
       "      <th>PVTEND</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-12-27 00:00:00</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-27 00:30:00</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-27 01:00:00</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-27 01:30:00</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-27 02:00:00</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       PTTEND  PUTEND  PVTEND\n",
       "timestamp                                    \n",
       "2000-12-27 00:00:00 -0.000003     0.0     0.0\n",
       "2000-12-27 00:30:00 -0.000003     0.0     0.0\n",
       "2000-12-27 01:00:00 -0.000003     0.0     0.0\n",
       "2000-12-27 01:30:00 -0.000003     0.0     0.0\n",
       "2000-12-27 02:00:00 -0.000003     0.0     0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targets = pd.DataFrame({'timestamp': timestamps,\n",
    "                           'PTTEND': pttend,\n",
    "                           'PUTEND': putend,\n",
    "                           'PVTEND': pvtend})\n",
    "df_targets.set_index('timestamp', inplace=True)\n",
    "df_targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use an initial split of 75% for training and 25% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_features, df_targets, test_size=0.25, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model by fitting to the training dataset. Then predict the labels using the test features, and get the RMSE compared against the test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6waMx-cMg71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:\n",
      "PTTEND    1.959330e-12\n",
      "PUTEND    0.000000e+00\n",
      "PVTEND    0.000000e+00\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = model.fit(x_train, y_train)\n",
    "\n",
    "# root mean square error \n",
    "rmse = np.sqrt(np.mean((model.predict(x_test) - y_test)**2))\n",
    "print(\"RMSE:\\n{}\".format(rmse))\n",
    "\n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a corresponding predictions dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVzN6_fWZDJn"
   },
   "source": [
    "Next we'll loop over each level of the features/labels datasets and use each of these in turn to train the model, which we'll then use to compute/predict corresponding labels. These predictions will be written into the predictions NetCDF (TODO along with the RMSE vs. original labels as separate/corresponding variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to extract a feature dataset for a specific level incorporating the feature variables (PS, T, U, and V).\n",
    "\n",
    "(Incorporating timestamps in case they're useful later when utilizing models that are relevant to timeseries, but in this first use case, i.e. with a simple linear regression model, this appears to be inconsequential/superfluous.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(ds_features,\n",
    "                     level_index,\n",
    "                     timestamps):\n",
    "    \n",
    "    # Create a DataFrame from the feature variables PS, T, U, and V.\n",
    "    ps = pd.Series(ds_features.variables['PS'].values[:, :, :].flatten())\n",
    "    t = pd.Series(ds_features.variables['T'].values[:, level_index, :, :].flatten())\n",
    "    u = pd.Series(ds_features.variables['U'].values[:, level_index, :, :].flatten())\n",
    "    v = pd.Series(ds_features.variables['V'].values[:, level_index, :, :].flatten())\n",
    "    df_features = pd.DataFrame({'timestamp': timestamps,\n",
    "                                'PS': ps,\n",
    "                                'T': t,\n",
    "                                'U': u,\n",
    "                                'V': v})\n",
    "    df_features.set_index('timestamp', inplace=True)\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to extract training and test datasets for a specific level and single target variable, using all feature variables (PS, T, U, and V).\n",
    "\n",
    "(Incorporating timestamps in case they're useful later when utilizing models that are relevant to timeseries, but in this first use case, i.e. with a simple linear regression model, this appears to be inconsequential/superfluous.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_test(ds_features,\n",
    "                       ds_labels,\n",
    "                       timestamps,\n",
    "                       level_index,\n",
    "                       label,\n",
    "                       test_percentage):\n",
    "    \n",
    "    # Create a DataFrame from the feature variables PS, T, U, and V.\n",
    "    df_features = extract_features(ds_features, level_index, timestamps)\n",
    "\n",
    "    # Create a labels DataFrame from the specified label variable.\n",
    "    target = pd.Series(ds_labels.variables[label].values[:, level_index, :, :].flatten())\n",
    "    df_target = pd.DataFrame({'timestamp': timestamps,\n",
    "                               label: target})\n",
    "    df_target.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # Pull training and test datasets from the features and target DataFrames.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_features, \n",
    "                                                        df_target, \n",
    "                                                        test_size=test_percentage,\n",
    "                                                        random_state=4)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine original shape of a single level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original shape of the variables (assumed to be the same for features and labels), \n",
    "# and use this to establish the shape we'll use for reshaping our predictions, which will \n",
    "# initally come out of the model as a flattened array.\n",
    "shape = ds_labels.variables['PVTEND'].values.shape\n",
    "shape_single_level = (shape[0], shape[2], shape[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model per level/label and predict values, writing the predictions to NetCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the predictions NetCDF as an xarray DataSet.\n",
    "ds_predictions = xr.open_dataset(path_predictions, lock=False)\n",
    "\n",
    "# Loop over levels, using a level index\n",
    "for lev_ix in range(ds_features.dims['lev']):\n",
    "    \n",
    "    # Loop over each label, in order to predict each in isolation\n",
    "    # TODO is this necessary/optimal, or can we just as well do all three simultaneously? TEST THIS\n",
    "    # i.e. is univariate any more accurate than multivariate when using sklearn's LRM?\n",
    "    for forcing_label in ['PTTEND', 'PUTEND', 'PVTEND']:\n",
    "    \n",
    "        # Extract training and testing datasets.\n",
    "        x_train, x_test, y_train, y_test = extract_train_test(ds_features,\n",
    "                                                              ds_labels,\n",
    "                                                              timestamps,\n",
    "                                                              lev_ix,\n",
    "                                                              forcing_label,\n",
    "                                                              0.25)\n",
    "        \n",
    "        # Train the model, then predict the relevant forcing tendencies using the trained model.\n",
    "        model.fit(x_train, y_train)\n",
    "        input_features = extract_features(ds_features, lev_ix, timestamps)\n",
    "        predictions = model.predict(input_features)\n",
    "        \n",
    "        # Write the prediction values into the NetCDF at the relevant level.\n",
    "        values = np.reshape(predictions, shape_single_level)\n",
    "        ds_predictions.variables[forcing_label].values[:, lev_ix, :, :] = values\n",
    "        \n",
    "# Write the predictions dataset back as a NetCDF, overwriting the previous copy.\n",
    "ds_predictions.to_netcdf('C:\\\\home\\\\cam_learn\\\\fv091x180L26_dry_HS.cam.PREDICTED.2000-12-27-00000_lowres.nc', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with computed results\n",
    "At this point we can compare the predictions against the computed labels to determine how accurately the linear regression model matched the results of the climate model. Once we're satisfied that the errors are marginal then we can use the LRM to predict labels using new inputs for the same features, and then test to se how closely the predictions match to the known results for those inputs. The goals are to develop an appropriate AI model (not ncessarily a linear regression model as is used above) and to determine the optimal parameters of the AI model where it can replace (and hopefully improve upon the results) of the existing climate model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "model_learn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
